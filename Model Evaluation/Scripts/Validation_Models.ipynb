{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataframe (100 FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col5_float  col7_float  col13_float  col15_float  col23_float  col27_float  \\\n",
      "0    0.000000    0.000000     0.590000     8.500000    15.000000        101.0   \n",
      "1   59.596924    0.000000     0.000000    22.092905    14.770059        100.0   \n",
      "2   59.596924    0.000000     0.000000    22.092905    14.770059        100.0   \n",
      "3   83.054077  389.560852     0.019531    22.092905    14.859506        100.0   \n",
      "4   83.054077  389.560852     0.019531    22.092905    14.859506        100.0   \n",
      "5   83.054077   12.898486     0.019531    22.092905    14.859506        100.0   \n",
      "6   83.054077   12.898486     0.019531    22.092905    14.859506        100.0   \n",
      "7   83.054077   12.898486     0.019531    22.092905    14.859506        100.0   \n",
      "8   83.054077   12.898486     0.019531    22.092905    14.859506        100.0   \n",
      "9   83.054077   12.898486     0.019531    22.092905    14.859506        100.0   \n",
      "\n",
      "   stableCruise_boolean  \n",
      "0                   0.0  \n",
      "1                   0.0  \n",
      "2                   0.0  \n",
      "3                   0.0  \n",
      "4                   0.0  \n",
      "5                   0.0  \n",
      "6                   0.0  \n",
      "7                   0.0  \n",
      "8                   0.0  \n",
      "9                   0.0  \n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "def perform_tasks_with_dask(file_pattern, num_workers=16, threads_per_worker=12, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Perform tasks using Dask, such as reading CSV files, configuring Dask, and printing the first 5 rows.\n",
    "\n",
    "    :param file_pattern: A file pattern to match CSV files.\n",
    "    :param num_workers: The number of Dask workers to use.\n",
    "    :param threads_per_worker: The number of threads per Dask worker.\n",
    "    :param use_gpu: Whether to use GPU for Dask computations.\n",
    "    :return: A Dask DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    dask.config.set(scheduler='threads', num_workers=num_workers, threads_per_worker=threads_per_worker, use_gpu=use_gpu)\n",
    "\n",
    "    df = dd.read_csv(file_pattern, assume_missing=True, header=0)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Specify the columns you want to keep\n",
    "    columns_to_keep = ['col5_float', 'col7_float', 'col13_float', 'col15_float', 'col23_float', 'col27_float', 'stableCruise_boolean'] \n",
    "    # Use square brackets to select and keep the specified columns\n",
    "    df_subset = df[columns_to_keep]\n",
    "    return df_subset\n",
    "    # numero_de_filas_exacto = len(df)\n",
    "    # print(\"Número de filas exacto:\", numero_de_filas_exacto)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_pattern = \"../../Data/V_Data/*.csv\"\n",
    "    df = perform_tasks_with_dask(file_pattern)\n",
    "\n",
    "    print(df.compute().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df.compute()[['col5_float', 'col7_float', 'col13_float', 'col15_float', 'col23_float', 'col27_float']]\n",
    "y_val = df.compute()['stableCruise_boolean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorization for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_variable(data, variable, new_column_name, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    \"\"\"\n",
    "    Realiza la winsorización de una variable en un conjunto de datos y la almacena en una nueva columna.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): El DataFrame que contiene los datos.\n",
    "        variable (str): El nombre de la variable que se va a winsorizar.\n",
    "        new_column_name (str): El nombre para la nueva columna que almacenará los datos winsorizados (si no se proporciona, se usará el nombre de la variable original con \"_winsorized\").\n",
    "        lower_percentile (float): Percentil inferior para la winsorización (valor predeterminado es 0.01).\n",
    "        upper_percentile (float): Percentil superior para la winsorización (valor predeterminado es 0.99).\n",
    "    \"\"\"\n",
    "    if new_column_name is None:\n",
    "        new_column_name = variable + \"_winsorized\"\n",
    "\n",
    "    lower_limit = data[variable].quantile(lower_percentile)\n",
    "    upper_limit = data[variable].quantile(upper_percentile)\n",
    "\n",
    "    data[new_column_name] = data[variable].clip(lower_limit, upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorize_variable(X_val, \"col7_float\", \"wzcol7_float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.drop('col7_float', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELO 1 (Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_filename = 'L2_Undersamplin_logistic_7var_model2.pkl'\n",
    "loaded_model1 = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model accuracy on the validation set: 0.8081465231092604\n",
      "Confusion Matrix for the validation set:\n",
      "[[10673753  2530542]\n",
      " [    6542    13234]]\n",
      "\n",
      "Classification Report for the validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.81      0.89  13204295\n",
      "         1.0       0.01      0.67      0.01     19776\n",
      "\n",
      "    accuracy                           0.81  13224071\n",
      "   macro avg       0.50      0.74      0.45  13224071\n",
      "weighted avg       1.00      0.81      0.89  13224071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set (if available)\n",
    "y_pred_val = loaded_model1.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the model on the validation set\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print the accuracy on the validation set\n",
    "print(\"\\nModel accuracy on the validation set:\", accuracy_val)\n",
    "\n",
    "# Display the confusion matrix and classification report for the validation set\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "class_report = classification_report(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion Matrix for the validation set:')\n",
    "print(conf_matrix)\n",
    "print('\\nClassification Report for the validation set:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2 (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_filename = 'L2_Undersamplin_xgboost_7var_model2.pkl'\n",
    "loaded_model2 = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set accuracy: 0.9621413103423295\n",
      "Confusion Matrix for the validation set:\n",
      "[[12706882   497413]\n",
      " [    3233    16543]]\n",
      "\n",
      "Classification Report for the validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98  13204295\n",
      "         1.0       0.03      0.84      0.06     19776\n",
      "\n",
      "    accuracy                           0.96  13224071\n",
      "   macro avg       0.52      0.90      0.52  13224071\n",
      "weighted avg       1.00      0.96      0.98  13224071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set (if available)\n",
    "y_pred_val = loaded_model2.predict(X_val)\n",
    "\n",
    "# Calculate the validation set accuracy\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print the validation set accuracy\n",
    "print(\"\\nValidation set accuracy:\", accuracy_val)\n",
    "\n",
    "# Display the confusion matrix and classification report for the validation set\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "class_report = classification_report(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion Matrix for the validation set:')\n",
    "print(conf_matrix)\n",
    "print('\\nClassification Report for the validation set:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 3 (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model3 = load_model('deepLearning_undersampling_model2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for the validation set:\n",
      "[[10359157  2845138]\n",
      " [    1600    18176]]\n",
      "\n",
      "Classification Report for the validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.78      0.88  13204295\n",
      "         1.0       0.01      0.92      0.01     19776\n",
      "\n",
      "    accuracy                           0.78  13224071\n",
      "   macro avg       0.50      0.85      0.45  13224071\n",
      "weighted avg       1.00      0.78      0.88  13224071\n",
      "\n",
      "Accuracy for the validation set: 0.7847305871240406\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "y_pred_val = (loaded_model3.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "# Calculate the accuracy of the model on the validation set\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Display the confusion matrix and classification report for the validation set\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "class_report = classification_report(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion Matrix for the validation set:')\n",
    "print(conf_matrix)\n",
    "print('\\nClassification Report for the validation set:')\n",
    "print(class_report)\n",
    "\n",
    "print('Accuracy for the validation set:', accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
